{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dec2de-9128-457a-a137-9fcaac7c86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a3d72-fe59-4a1c-b015-02a2a9dc6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "# Conditional skipping of https://kioku-space.com/en/jupyter-skip-execution/\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)\n",
    "\n",
    "get_ipython().register_magic_function(skip_if, 'cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff80a1-64cd-488b-b7fe-865bf58d44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if importlib.util.find_spec('Jinja2') is not None\n",
    "%pip install Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e410ec-a5c6-45e3-9cd2-119cdc84411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if importlib.util.find_spec('nbformat') is not None\n",
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501333f-a301-4bf9-8d10-be01afade5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537c8d5-86a9-421f-8ff8-09fcba4e2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793bb32-8cd4-47b3-858c-5dc6d04d2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('raw_data/yqrStops.json', 'r') as f:\n",
    "        stop_data = json.load(f)\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(\"Invalid JSON\", e)\n",
    "\n",
    "stop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd86e64-a627-4a8a-8ee8-d70ef46e46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops = pd.json_normalize(stop_data)\n",
    "df_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109971f-dffd-4c03-be64-f120bb1ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops = pd.json_normalize(stop_data['features'])\n",
    "df_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d9d1b-c3fe-4ad5-9cbe-45c8b2ff1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bus Stop Data Types\")\n",
    "print(df_stops.dtypes[df_stops.columns[:8]])\n",
    "print(\"---------------------------------\")\n",
    "print(f\"\\nSample LAT values: {df_stops['attributes.LAT'].head(2).tolist()}\")\n",
    "print(\"Bus Stop Missing Values\")\n",
    "missing = df_stops.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "print(\"---------------------------------\")\n",
    "print(\"Bus Stop Duplicates\")\n",
    "duplicates = df_stops.duplicated(subset=['attributes.STOP_ID']).sum()\n",
    "print(f\"Duplicate stop IDs: {duplicates}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6061b1-58d3-47ac-9a17-4e3f483c1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======Sample Raw Bus Stop Data=======\")\n",
    "df_stops[['attributes.STOP_ID','attributes.ONSTREET', 'attributes.ATSTREET', 'attributes.LAT', 'attributes.LON']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75497628-57ed-41d8-809b-fb5f8bb96b7f",
   "metadata": {},
   "source": [
    " # Cleaning Bus Stop data using Data Wrangler extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519274a1-f329-4ba9-8c88-788bfa37c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df_stops):\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.ATSTREET'\n",
    "    df_stops['attributes.ATSTREET'] = df_stops['attributes.ATSTREET'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.ATSTREET'\n",
    "    df_stops['attributes.ATSTREET'] = df_stops['attributes.ATSTREET'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.LON'\n",
    "    df_stops['attributes.LON'] = df_stops['attributes.LON'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.LAT'\n",
    "    df_stops['attributes.LAT'] = df_stops['attributes.LAT'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'stop_id'\n",
    "    df_stops['attributes.STOP_ID'] = df_stops['attributes.STOP_ID'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.STOP_NAME'\n",
    "    df_stops['attributes.STOP_NAME'] = df_stops['attributes.STOP_NAME'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.STOP_NAME'\n",
    "    df_stops['attributes.STOP_NAME'] = df_stops['attributes.STOP_NAME'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.GLOBALID'\n",
    "    df_stops['attributes.GLOBALID'] = df_stops['attributes.GLOBALID'].str.strip()\n",
    "    # Replace missing values with \"DOROTHY ST (SB)\" in column: 'attributes.ATSTREET'\n",
    "    df_stops = df_stops.fillna({'attributes.ATSTREET':\"DOROTHY ST (SB)\"})\n",
    "    # Replace all instances of \"1060 DOROTHY ST (SB)\" with \"DOROTHY ST\" in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.replace(\"1060 DOROTHY ST (SB)\", \"DOROTHY ST\", case=False, regex=False)\n",
    "    # Rename column 'attributes.ONSTREET' to 'on_street'\n",
    "    df_stops = df_stops.rename(columns={'attributes.ONSTREET': 'on_street'})\n",
    "    # Rename column 'attributes.ATSTREET' to 'at_street'\n",
    "    df_stops = df_stops.rename(columns={'attributes.ATSTREET': 'at_street'})\n",
    "    # Rename column 'attributes.LON' to 'lon'\n",
    "    df_stops = df_stops.rename(columns={'attributes.LON': 'lon'})\n",
    "    # Rename column 'attributes.LAT' to 'lat'\n",
    "    df_stops = df_stops.rename(columns={'attributes.LAT': 'lat'})\n",
    "    # Rename column 'stop_id' to 'stop_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.STOP_ID': 'stop_id'})\n",
    "    # Rename column 'attributes.STOP_NAME' to 'stop_name'\n",
    "    df_stops = df_stops.rename(columns={'attributes.STOP_NAME': 'stop_name'})\n",
    "    # Rename column 'attributes.GLOBALID' to 'global_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.GLOBALID': 'global_id'})\n",
    "    # Rename column 'attributes.OBJECTID' to 'object_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.OBJECTID': 'object_id'})\n",
    "    return df_stops\n",
    "\n",
    "clean_stops = clean_data(df_stops.copy())\n",
    "clean_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82608f0-9c4b-48cc-b3c0-061107c37ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('raw_data/yqrRoutes.json', 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "    print(\"âœ“ Loaded routes data\")\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(\"Invalid JSON\", e)\n",
    "\n",
    "df_routes = pd.json_normalize(routes_data['features'])\n",
    "df_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286e95c-b3c1-40bc-b1a1-f5f554ddeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bus Route Data Types\")\n",
    "print(df_routes.dtypes[df_routes.columns[:8]])\n",
    "print(\"---------------------------------\")\n",
    "print(\"Bus Route Missing Values\")\n",
    "missing = df_routes.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "print(\"---------------------------------\")\n",
    "print(\"Bus Route Duplicates\")\n",
    "duplicates = df_routes.duplicated(subset=['attributes.ROUTE_ID']).sum()\n",
    "print(f\"Duplicate stop IDs: {duplicates}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd783a-6bf5-4708-9d22-d792d534ca03",
   "metadata": {},
   "source": [
    " # Cleaning Bus Route Data using Data Wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea14b35-5362-4c3c-8069-046b9233d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df_routes):\n",
    "    # Remove leading and trailing whitespace in columns: 'attributes.ROUTE_NAME', 'attributes.ROUTE_NUM' and 4 other columns\n",
    "    df_routes['attributes.ROUTE_NAME'] = df_routes['attributes.ROUTE_NAME'].str.strip()\n",
    "    df_routes['attributes.ROUTE_NUM'] = df_routes['attributes.ROUTE_NUM'].str.strip()\n",
    "    df_routes['attributes.ROUTE_ID'] = df_routes['attributes.ROUTE_ID'].str.strip()\n",
    "    df_routes['attributes.SHAPE_ID'] = df_routes['attributes.SHAPE_ID'].str.strip()\n",
    "    # add a hashtag in front of the route colour hex values \n",
    "    df_routes['attributes.ROUTE_COLOR'] = '#' + (df_routes['attributes.ROUTE_COLOR'].str.strip()).astype(str)\n",
    "    # Convert text to uppercase in column: 'attributes.ROUTE_NAME'\n",
    "    df_routes['attributes.ROUTE_NAME'] = df_routes['attributes.ROUTE_NAME'].str.upper()\n",
    "    # Replace missing values with \"FFFFFF\" in column: 'attributes.ROUTE_TEXT_COLOR'\n",
    "    df_routes = df_routes.fillna({'attributes.ROUTE_TEXT_COLOR': \"FFFFFF\"})\n",
    "    # add a hashtag in front of the route text colour hex values\n",
    "    df_routes['attributes.ROUTE_TEXT_COLOR'] = \"#\"+ (df_routes['attributes.ROUTE_TEXT_COLOR'].str.strip()).astype(str)\n",
    "     # Rename column 'attributes.SHAPE.LEN' to 'shape_length'\n",
    "    df_routes = df_routes.rename(columns={'attributes.SHAPE.LEN': 'shape_length'})\n",
    "    # Rename column 'attributes.ROUTE_NAME' to 'route_name'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_NAME': 'route_name'})\n",
    "    # Rename column 'attributes.ROUTE_NUM' to 'route_num'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_NUM': 'route_num'})\n",
    "    # Rename column 'attributes.ROUTE_ID' to 'route_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_ID': 'route_id'})\n",
    "    # Rename column 'attributes.SHAPE_ID' to 'shape_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.SHAPE_ID': 'shape_id'})\n",
    "    # Rename column 'attributes.ROUTE_COLOR' to 'route_color'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_COLOR': 'route_color'})\n",
    "    # Rename column 'attributes.ROUTE_TEXT_COLOR' to 'route_text_color'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_TEXT_COLOR': 'route_text_color'})\n",
    "    # Rename column 'geometry.paths' to 'geometry_paths'\n",
    "    df_routes = df_routes.rename(columns={'geometry.paths': 'geometry_paths'})\n",
    "    # Rename column 'attributes.OBJECTID' to 'object_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.OBJECTID': 'object_id'})\n",
    "    return df_routes\n",
    "\n",
    "clean_routes = clean_data(df_routes.copy())\n",
    "clean_routes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef730def-74d3-43cb-91d1-acb007fe9592",
   "metadata": {},
   "source": [
    " # Loading GTFS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d9eb9-824b-4edd-bae4-e133972eb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gtfs data\n",
    "stops_gtfs = pd.read_csv('raw_data/gtfs_data/stops.txt')\n",
    "routes_gtfs = pd.read_csv('raw_data/gtfs_data/routes.txt')\n",
    "trips_gtfs = pd.read_csv('raw_data/gtfs_data/trips.txt')\n",
    "times_gtfs = pd.read_csv('raw_data/gtfs_data/stop_times.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8977f8-d115-480a-9ebb-b1d4d17941a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_gtfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cda745-707e-46e9-aede-ecec3d08a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(stops_gtfs):\n",
    "    # Convert text to uppercase in column: 'stop_name'\n",
    "    stops_gtfs['stop_name'] = stops_gtfs['stop_name'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'stop_name'\n",
    "    stops_gtfs['stop_name'] = stops_gtfs['stop_name'].str.strip()\n",
    "    return stops_gtfs\n",
    "\n",
    "stops_gtfs_clean = clean_data(stops_gtfs.copy())\n",
    "stops_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0779f1a-49e0-41cd-bd9b-068b3677566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_gtfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fed616-397e-4a49-873e-da48d931ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(routes_gtfs):\n",
    "    # Convert text to uppercase in column: 'route_long_name'\n",
    "    routes_gtfs['route_long_name'] = routes_gtfs['route_long_name'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'route_long_name'\n",
    "    routes_gtfs['route_long_name'] = routes_gtfs['route_long_name'].str.strip()\n",
    "    return routes_gtfs\n",
    "\n",
    "routes_gtfs_clean = clean_data(routes_gtfs.copy())\n",
    "routes_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becdc28-e92c-43ba-8e08-373a2940ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_gtfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227c136-d9b5-4b22-acef-93fdaf7bb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(trips_gtfs):\n",
    "    # Remove leading and trailing whitespace in columns: 'route_id', 'service_id' and 2 other columns\n",
    "    trips_gtfs['route_id'] = trips_gtfs['route_id'].str.strip()\n",
    "    trips_gtfs['service_id'] = trips_gtfs['service_id'].str.strip()\n",
    "    trips_gtfs['trip_id'] = trips_gtfs['trip_id'].str.strip()\n",
    "    trips_gtfs['trip_headsign'] = trips_gtfs['trip_headsign'].str.strip()\n",
    "    # Convert text to uppercase in columns: 'service_id', 'trip_id', 'trip_headsign'\n",
    "    trips_gtfs['service_id'] = trips_gtfs['service_id'].str.upper()\n",
    "    trips_gtfs['trip_id'] = trips_gtfs['trip_id'].str.upper()\n",
    "    trips_gtfs['trip_headsign'] = trips_gtfs['trip_headsign'].str.upper()\n",
    "    return trips_gtfs\n",
    "\n",
    "trips_gtfs_clean = clean_data(trips_gtfs.copy())\n",
    "trips_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc83e4-26a3-4378-a7a1-8c42dce8d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_gtfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aeab8e-4c8d-45c1-9107-76663710426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(times_gtfs):\n",
    "    # Convert text to uppercase in column: 'trip_id'\n",
    "    times_gtfs['trip_id'] = times_gtfs['trip_id'].str.upper()\n",
    "    # Replace '24' in hour position with '00'\n",
    "    times_gtfs['arrival_time'] = times_gtfs['arrival_time'].str.replace(r'^24', '00', regex=True)\n",
    "    times_gtfs['departure_time'] = times_gtfs['departure_time'].str.replace(r'^24', '00', regex=True)\n",
    "    # Remove leading and trailing whitespace in columns: 'trip_id', 'arrival_time', 'departure_time'\n",
    "    times_gtfs['trip_id'] = times_gtfs['trip_id'].str.strip()\n",
    "    times_gtfs['arrival_time'] = times_gtfs['arrival_time'].str.strip()\n",
    "    times_gtfs['departure_time'] = times_gtfs['departure_time'].str.strip()\n",
    "    return times_gtfs\n",
    "\n",
    "times_gtfs_clean = clean_data(times_gtfs.copy())\n",
    "times_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a97076-afac-42c5-95df-ccd169e4efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse time columns\n",
    "times_gtfs_clean['arrival_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['arrival_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "times_gtfs_clean['departure_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['departure_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Derive hour of day\n",
    "times_gtfs_clean['arrival_hour'] = times_gtfs_clean['arrival_datetime'].dt.hour\n",
    "times_gtfs_clean['arrival_minute'] = times_gtfs_clean['arrival_datetime'].dt.minute\n",
    "times_gtfs_clean['departure_hour'] = times_gtfs_clean['departure_datetime'].dt.hour\n",
    "times_gtfs_clean['departure_minute'] = times_gtfs_clean['departure_datetime'].dt.minute\n",
    "\n",
    "print(\"Sample parsed times:\")\n",
    "times_gtfs_clean[['arrival_time', 'arrival_datetime', 'arrival_hour', 'arrival_minute', 'departure_datetime', 'departure_time', 'departure_hour', 'departure_minute']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26905c87-ac10-484c-94fb-27f56fc77563",
   "metadata": {},
   "source": [
    " # Imputation\n",
    " ## Find missing bus stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cb038-ddc1-47f3-b043-1d30fca41469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_stops))\n",
    "print(len(stops_gtfs_clean))\n",
    "if len(stops_gtfs_clean) > len(clean_stops):\n",
    "    print(f\"There are {len(stops_gtfs_clean)-len(clean_stops)} missing stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d309691-3b0c-4793-a1fc-133999326ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the join keys have the same data type\n",
    "clean_stops['stop_id'] = clean_stops['stop_id'].astype(str)\n",
    "stops_gtfs_clean['stop_id'] = stops_gtfs_clean['stop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95ad21-eabc-4644-85e5-a90e833dbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the missing stops\n",
    "missing_stops = stops_gtfs_clean[~stops_gtfs_clean['stop_id'].isin(clean_stops['stop_id'])]\n",
    "len(missing_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05bfd8-03f9-4d98-aad6-bd981c42503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stops = pd.concat([clean_stops, missing_stops], ignore_index=True, sort=False)\n",
    "merged_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174dea9-e9db-46f4-9abd-8662ca18016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(missing_stops), \"new stops added\")\n",
    "merged_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa2590-5541-4e52-90d7-e2e84f781e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(merged_stops):\n",
    "    for index, stop in merged_stops.iterrows():\n",
    "        if pd.isna(stop['on_street']):\n",
    "            merged_stops.at[index, 'on_street'] = (\n",
    "                str(merged_stops.at[index, 'stop_name']).split(' @')[0]\n",
    "            )\n",
    "        if pd.isna(stop['at_street']):\n",
    "            merged_stops.at[index, 'at_street'] = (\n",
    "                str(merged_stops.at[index, 'stop_name']).split('@ ')[-1]\n",
    "            )\n",
    "    return merged_stops\n",
    "\n",
    "merged_stops_clean = clean_data(merged_stops.copy())\n",
    "merged_stops_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8df3bd-30f1-40a5-81e1-26988446918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from the Plotly documentation https://plotly.com/python/tile-scatter-maps/#multiple-markers\n",
    "stop_fig = go.Figure(go.Scattermap(\n",
    "    lat=clean_stops['lat'],\n",
    "    lon=clean_stops['lon'],\n",
    "    mode='markers',\n",
    "    marker=go.scattermap.Marker(size=9),\n",
    "    text=clean_stops['stop_name'],\n",
    "))\n",
    "\n",
    "stop_fig.update_layout(\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    map=dict(\n",
    "        bearing=0,\n",
    "        center=dict(lat=50.447992743219615, lon=-104.61228441057489),\n",
    "        pitch=0,\n",
    "        zoom=10\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bc551-35f3-4c15-ae2c-84b0587ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if importlib.util.find_spec('pyproj') is not None\n",
    "%pip install pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480b6b7-4f49-484d-9c43-f8415e1d6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a10bfe-f534-4ec3-b5c2-2890c452c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer to convert from UTM to lat/lon\n",
    "transformer = Transformer.from_crs(\"EPSG:26913\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "route_idx = 0\n",
    "\n",
    "route_name = clean_routes['route_name'].iloc[route_idx]\n",
    "route_geometry = clean_routes['geometry_paths'].iloc[route_idx]\n",
    "route_colour = clean_routes['route_color'].iloc[route_idx]\n",
    "route_text_colour = clean_routes['route_text_color'].iloc[route_idx]\n",
    "\n",
    "# Combine all paths into single lists\n",
    "all_lons = []\n",
    "all_lats = []\n",
    "\n",
    "for path in route_geometry:\n",
    "    for coordinate in path:\n",
    "        lon, lat = transformer.transform(coordinate[0], coordinate[1])\n",
    "        all_lons.append(lon)\n",
    "        all_lats.append(lat)\n",
    "    \n",
    "    # Add None to separate path segments (prevents connecting them)\n",
    "    all_lons.append(None)\n",
    "    all_lats.append(None)\n",
    "\n",
    "# Add as ONE trace\n",
    "stop_fig.add_trace(go.Scattermap(\n",
    "    lon=all_lons,\n",
    "    lat=all_lats,\n",
    "    mode='lines',\n",
    "    line=dict(width=3, color=route_colour),\n",
    "    name=route_name,\n",
    "    hovertemplate=f'<b>{route_name}</b><extra></extra>'\n",
    "))\n",
    "\n",
    "stop_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a73b7a-dd48-43df-88f4-5d309bc580e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: Bus Stops by Region\n",
    "\n",
    "city_center_lon = -104.618\n",
    "city_center_lat = 50.447\n",
    "clean_stops['region'] = ''\n",
    "\n",
    "for stop in range(len(clean_stops)):\n",
    "    if float(clean_stops['lat'].iloc[stop]) > city_center_lat:\n",
    "        if float(clean_stops['lon'].iloc[stop]) > city_center_lon:\n",
    "            clean_stops.at[stop, 'region'] = \"NE\"\n",
    "        else:\n",
    "            clean_stops.at[stop, 'region'] = \"NW\"\n",
    "    else:\n",
    "        if float(clean_stops['lon'].iloc[stop]) > city_center_lon:\n",
    "            clean_stops.at[stop, 'region'] = \"SE\"\n",
    "        else:\n",
    "            clean_stops.at[stop, 'region'] = \"SW\"\n",
    "\n",
    "clean_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d655f9d-dca5-4ba5-81c3-740a06652d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwStops = clean_stops[clean_stops['region'] == 'NW']\n",
    "neStops = clean_stops[clean_stops['region'] == 'NE']\n",
    "swStops = clean_stops[clean_stops['region'] == 'SW']\n",
    "seStops = clean_stops[clean_stops['region'] == 'SE']\n",
    "\n",
    "nwStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e458df8-0a7d-4b3f-a470-2749022687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137f9b9-1cd6-49fc-9a7e-e9d6883cc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "swStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99044f8e-91e1-4e87-ba62-db379271382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeece9c-eee6-4227-8ea1-808e43b85c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(clean_stops) == len(nwStops)+len(neStops) + len(swStops) + len(seStops):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddcaf4-86c3-415f-99c7-ec5faaf69eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total distance of each bus route\n",
    "\n",
    "# convert shape_length into km\n",
    "clean_routes['route_distance_km'] = clean_routes['shape_length'] / 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0335fd-b6bd-40f5-a624-a37e1ea6630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_routes[['route_num', 'route_distance_km']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a77704-7c15-4c71-a656-d721c3d1bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate conversion: ~111 km per degree latitude, ~85 km per degree longitude at this latitude\n",
    "# Euclidean distance\n",
    "clean_stops['distance_from_center_km'] = np.sqrt(\n",
    "    ((clean_stops['lat'].astype(float) - city_center_lat) * 111)**2 +  # 111 km per degree lat\n",
    "    ((clean_stops['lon'].astype(float) - city_center_lon) * 85)**2     # ~85 km per degree lon at this latitude\n",
    ")\n",
    "\n",
    "print(\"Distance statistics (km):\")\n",
    "print(clean_stops['distance_from_center_km'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8871a06-e7ed-46fe-b3ae-820f96a5a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation - Region Summary\n",
    "\n",
    "region_summary = clean_stops.groupby('region').agg({\n",
    "    'stop_id': 'count',\n",
    "    'distance_from_center_km': ['mean', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "region_summary.columns = ['region', 'num_stops', 'avg_distance_km', 'max_distance_km']\n",
    "print(\"Stops by region:\")\n",
    "print(region_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc4ea3-f20a-4f4b-890b-ca6b896f6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "print(\"=== OUTLIERS ===\")\n",
    "print(f\"Distance from center - outliers beyond 15km:\")\n",
    "outliers = clean_stops[clean_stops['distance_from_center_km'] > 15]\n",
    "print(f\"Found {len(outliers)} stops beyond 15km\")\n",
    "print(outliers[['stop_name', 'distance_from_center_km']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9427ef-8520-476c-b6f7-20cd62641f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinalities\n",
    "print(\"\\n=== FIX - CARDINALITIES ===\")\n",
    "print(f\"Unique stops: {['stop_id'].nunique()}\")\n",
    "print(f\"Unique routes: {routes_gtfs_clean['route_id'].nunique()}\")\n",
    "print(f\"Unique regions: {clean_stops['region'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa330c-bf4b-4024-a5ff-9b13dad983be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape: Routes by Region\n",
    "times_gtfs_clean['trip_id'] = times_gtfs_clean['trip_id'].astype(str)\n",
    "trips_gtfs_clean['trip_id'] = trips_gtfs_clean['trip_id'].astype(str)\n",
    "times_gtfs_clean['stop_id'] = times_gtfs_clean['stop_id'].astype(str)\n",
    "clean_stops['stop_id'] = clean_stops['stop_id'].astype(str)\n",
    "trips_gtfs_clean['route_id'] = trips_gtfs_clean['route_id'].astype(str)\n",
    "\n",
    "route_stops = (\n",
    "    times_gtfs_clean\n",
    "    .merge(trips_gtfs_clean[['trip_id', 'route_id']], on='trip_id')\n",
    "    .merge(clean_stops[['stop_id', 'region']], left_on='stop_id', right_on='stop_id')\n",
    "    .groupby(['route_id', 'region'])\n",
    "    .size()\n",
    "    .reset_index(name='stop_count')\n",
    ")\n",
    "\n",
    "# Pivot wider\n",
    "route_region_pivot = route_stops.pivot(\n",
    "    index='route_id', \n",
    "    columns='region', \n",
    "    values='stop_count'\n",
    ").fillna(0)\n",
    "\n",
    "print(\"FIX - Routes by region (pivoted):\")\n",
    "print(route_region_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4008d-689b-40f0-ad1a-36e78cb157de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and After Evidence\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BEFORE (Raw Data)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Stop rows: {len(df_stops)}\")\n",
    "print(f\"Route rows: {len(df_routes)}\")\n",
    "print(f\"Missing ATSTREET: {df_stops['attributes.ATSTREET'].isnull().sum()}\")\n",
    "print(f\"String coordinates: {df_stops['attributes.LAT'].dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"AFTER (Cleaned & Transformed)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Stop rows: {len(clean_stops)} (added {len(missing_stops)} from GTFS)\")\n",
    "print(f\"Route rows: {len(clean_routes)}\")\n",
    "print(f\"Missing ATSTREET: {clean_stops['at_street'].isnull().sum()}\")\n",
    "print(f\"Numeric coordinates: {clean_stops['lat'].dtype}\")\n",
    "print(f\"New features: region, distance_from_center_km\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
