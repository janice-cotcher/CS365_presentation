{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bb660-4438-4400-8d46-68697122c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "# Conditional skipping of https://kioku-space.com/en/jupyter-skip-execution/\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)\n",
    "\n",
    "get_ipython().register_magic_function(skip_if, 'cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3f55c-c773-4664-8499-de67aa2c5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mercury as mr\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0a257-0b0b-4af4-99cf-f69dd36a00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.App(\n",
    "    title=\"CS 365: Transit Regina Data Wrangling\", \n",
    "    description=\"Cleaning and transforming Transit Regina data for analysis\", \n",
    "    show_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fc377-ef0a-402d-99b3-33ad59be561e",
   "metadata": {},
   "source": [
    "  # CS 365 Final Project: Transit Regina Data Wrangling\n",
    "\n",
    " ## Janice Cotcher\n",
    " ## December 5, 2025\n",
    "\n",
    " **Data Source:** City of Regina Open Data Portal\n",
    "\n",
    " **Dataset:** Transit Stops and Routes (November 20, 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899b0bf-135c-4b54-87db-5f534ffa55ce",
   "metadata": {},
   "source": [
    " ## Dataset Source & License\n",
    "\n",
    "  **Source:** [City of Regina Open Data Portal](https://open.regina.ca)\n",
    "  - Bus Stop Locations (yqrStops.json)\n",
    "  - Transit Routes (yqrRoutes.json)\n",
    "  - General Transit Feed Specification(routes.txt, stops.txt, trips.txt, stop_times.txt)\n",
    "\n",
    "  **License:** Open Government License - Regina\n",
    " - Allows educational and commercial use\n",
    " - No Personally Identifiable Information - only public infrastructure data\n",
    "\n",
    "  **Why this matters:** Understanding transit accessibility and route coverage for urban planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd3751-38b9-4df9-a826-7e9cc060c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('open_regina_license.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5749a-9caf-4d62-aa3b-eb875649c8cc",
   "metadata": {},
   "source": [
    " ## Why I used a Jupyter Notebook\n",
    "\n",
    " - Technical Skills\n",
    " - Dataset size[1]\n",
    " - Workflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " [1] CS 365 - Data Cleaning: Concepts & Algorithms, Lecture 6: September 15, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873901e1-296e-40ea-aa1d-074b4d15576d",
   "metadata": {},
   "source": [
    " ## Imported Python Libraries\n",
    "\n",
    " ```python\n",
    " import numpy as np # scientific computing for large, multi-dimensional arrays\n",
    " from matplotlib import pyplot as plt # creates static, animated, and interactive visualizations\n",
    " import pandas as pd #creates 2D, size-mutable, heterogeneous tables called data frames\n",
    " import json # read and write json files\n",
    " import plotly.graph_objects as go # interactive graphics like maps\n",
    " from pyproj import Transformer # cartography and coordinate transformations\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde94b8f-d84f-4da7-a068-c1ed522459cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e5c06-570f-4eba-90fc-9c07913260a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"## Raw Data Snapshot: Bus Stops\"\"\")\n",
    "Image('stops_raw_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2981cfd-a1af-4d8a-89ae-7d57f19bed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "```python\n",
    "# Load JSON data\n",
    "try:\n",
    "    with open('raw_data/yqrStops.json', 'r') as f:\n",
    "        stop_data = json.load(f)\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(\"Invalid JSON\", e)\n",
    "\n",
    "# Normalize nested JSON structure\n",
    "df_stops = pd.json_normalize(stop_data['features'])\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0cd95-b2d0-40bc-945d-d65cb41cc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop data\n",
    "try:\n",
    "    with open('raw_data/yqrStops.json', 'r') as f:\n",
    "        stop_data = json.load(f)\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(\"Invalid JSON\", e)\n",
    "\n",
    "df_stops = pd.json_normalize(stop_data['features'])\n",
    "\n",
    "mr.Markdown(f\"**Loaded bus stops:**{len(df_stops)}\")\n",
    "df_stops.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb92b8-8a6d-43e7-a12a-1df49f564c60",
   "metadata": {},
   "source": [
    " # Data Profiling: Quality Assessment - Stops\n",
    "\n",
    " - **Data types** - Are coordinates stored correctly?\n",
    " - **Missing values** - Which columns have gaps?\n",
    " - **Duplicates** - Any duplicate stop IDs?\n",
    " - **Outliers** - Any stops in unexpected locations?\n",
    " - **Cardinalities** - How many unique stops?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e276bc-275f-4a5e-abd7-575d2063a488",
   "metadata": {},
   "source": [
    " ### BUS STOP DATA TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fab3b-1822-4aaf-81f7-37b1fa224001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stops.dtypes[df_stops.columns[:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc5741-78a7-4423-b672-84935e9f9c83",
   "metadata": {},
   "source": [
    " ## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c9073-ea03-4f40-bcb7-ad2fc5b21c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df_stops.isnull().sum()\n",
    "print(f\"Number of Missing Values: {len(missing[missing > 0])}\")\n",
    "print(\"Missing Values\")\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c0b0f-fb05-4166-893f-6575797aea40",
   "metadata": {},
   "source": [
    " ## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8f434-dacb-4799-8445-35edfd46a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_stops.duplicated(subset=['attributes.STOP_ID']).sum()\n",
    "print(f\"Number of Duplicate Stops - checked by stop ID: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a7b7-a474-49e2-ab53-e6cfe57aff25",
   "metadata": {},
   "source": [
    " ### Sample Latitude Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede8339-9583-4543-a609-a795ca4980e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops['attributes.LAT'].head(5).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81809d-7b24-4282-a902-1c9f0f803a9f",
   "metadata": {},
   "source": [
    " # Cleaning: Bus Stop Text Standardization & Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5455f44-0d53-4108-a9df-2477145adeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "## Problems identified\n",
    "- Inconsistent text formatting (mixed case, whitespace)\n",
    "- Coordinates stored as strings instead of numeric latitude and longitude\n",
    "- Missing values in street names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812b61d-fb3f-45b8-bc19-c21804801005",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Sample Solutions:**\n",
    "```python\n",
    "# Text standardization\n",
    "df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.strip().str.upper()\n",
    "df_stops['attributes.ATSTREET'] = df_stops['attributes.ATSTREET'].str.strip().str.upper()\n",
    "...\n",
    "# Missing value imputation\n",
    "df_stops = df_stops.fillna({'attributes.ATSTREET': \"DOROTHY ST (SB)\"})\n",
    "\n",
    "# Data correction\n",
    "df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.replace(\n",
    "    \"1060 DOROTHY ST (SB)\", \"DOROTHY ST\", regex=False\n",
    ")\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d74082-6fce-4deb-ae14-914642bbf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df_stops):\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.ATSTREET'\n",
    "    df_stops['attributes.ATSTREET'] = df_stops['attributes.ATSTREET'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.ATSTREET'\n",
    "    df_stops['attributes.ATSTREET'] = df_stops['attributes.ATSTREET'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.LON'\n",
    "    df_stops['attributes.LON'] = df_stops['attributes.LON'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.LAT'\n",
    "    df_stops['attributes.LAT'] = df_stops['attributes.LAT'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'stop_id'\n",
    "    df_stops['attributes.STOP_ID'] = df_stops['attributes.STOP_ID'].str.strip()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.STOP_NAME'\n",
    "    df_stops['attributes.STOP_NAME'] = df_stops['attributes.STOP_NAME'].str.strip()\n",
    "    # Convert text to uppercase in column: 'attributes.STOP_NAME'\n",
    "    df_stops['attributes.STOP_NAME'] = df_stops['attributes.STOP_NAME'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'attributes.GLOBALID'\n",
    "    df_stops['attributes.GLOBALID'] = df_stops['attributes.GLOBALID'].str.strip()\n",
    "    # Replace missing values with \"DOROTHY ST (SB)\" in column: 'attributes.ATSTREET'\n",
    "    df_stops = df_stops.fillna({'attributes.ATSTREET':\"DOROTHY ST (SB)\"})\n",
    "    # Replace all instances of \"1060 DOROTHY ST (SB)\" with \"DOROTHY ST\" in column: 'attributes.ONSTREET'\n",
    "    df_stops['attributes.ONSTREET'] = df_stops['attributes.ONSTREET'].str.replace(\"1060 DOROTHY ST (SB)\", \"DOROTHY ST\", case=False, regex=False)\n",
    "    # Rename column 'attributes.ONSTREET' to 'on_street'\n",
    "    df_stops = df_stops.rename(columns={'attributes.ONSTREET': 'on_street'})\n",
    "    # Rename column 'attributes.ATSTREET' to 'at_street'\n",
    "    df_stops = df_stops.rename(columns={'attributes.ATSTREET': 'at_street'})\n",
    "    # Rename column 'attributes.LON' to 'lon'\n",
    "    df_stops = df_stops.rename(columns={'attributes.LON': 'lon'})\n",
    "    # Rename column 'attributes.LAT' to 'lat'\n",
    "    df_stops = df_stops.rename(columns={'attributes.LAT': 'lat'})\n",
    "    # Rename column 'stop_id' to 'stop_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.STOP_ID': 'stop_id'})\n",
    "    # Rename column 'attributes.STOP_NAME' to 'stop_name'\n",
    "    df_stops = df_stops.rename(columns={'attributes.STOP_NAME': 'stop_name'})\n",
    "    # Rename column 'attributes.GLOBALID' to 'global_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.GLOBALID': 'global_id'})\n",
    "    # Rename column 'attributes.OBJECTID' to 'object_id'\n",
    "    df_stops = df_stops.rename(columns={'attributes.OBJECTID': 'object_id'})\n",
    "    return df_stops\n",
    "\n",
    "clean_stops= clean_data(df_stops.copy())\n",
    "clean_stops.head()\n",
    "\n",
    "mr.Markdown(f\"**Cleaned {len(clean_stops)} bus stops**\")\n",
    "clean_stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc87e5-ad16-400b-8d9f-6e51d987eb6c",
   "metadata": {},
   "source": [
    " ## Raw Data Snapshot: Bus Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1decc00-de92-4162-ad1b-2a5f1f3ae379",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('routes_raw_data_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde89bdd-3aec-4fc1-b19a-14bc2c8b36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('routes_raw_data_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706dbcc-2acd-4e67-b676-d6bbb8f51d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"## Over 90K lines, ~4.9MB\"\"\")\n",
    "Image('routes_raw_data_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c5875-d83a-4e4e-939e-1efe049185df",
   "metadata": {},
   "source": [
    "  ## Loading Routes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617a178-345b-447a-b0a9-af05ecbea0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "```python\n",
    "# Load routes data\n",
    "with open('raw_data/yqrRoutes.json', 'r') as f:\n",
    "    routes_data = json.load(f)\n",
    "\n",
    "df_routes = pd.json_normalize(routes_data['features'])\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736bb21-07f1-41db-ba9a-b62bd4cf9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load routes\n",
    "try:\n",
    "    with open('raw_data/yqrRoutes.json', 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(\"Invalid JSON\", e)\n",
    "\n",
    "df_routes = pd.json_normalize(routes_data['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617a1c4-78fa-495d-aa30-4e37b6110e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87029e-1c3b-4b19-afce-168a4eef182b",
   "metadata": {},
   "source": [
    " # Data Profile: Quality Assessment - Routes\n",
    " - **Data types** - Are coordinates stored correctly?\n",
    " - **Missing values** - Which columns have gaps?\n",
    " - **Duplicates** - Any duplicate route IDs?\n",
    " - **Outliers** - Any routes in unexpected locations?\n",
    " - **Cardinalities** - How many unique routes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4598e8-9eb3-4b64-897f-7fc8f0cfba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"## Route Data Types\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaede31-0263-419e-a930-54be100bb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_routes.dtypes[df_routes.columns[:6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762168c-9a6b-4067-a732-e265531d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"## Missing Values\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d352cb2-93a7-4550-bf68-b85e10d9ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df_routes.isnull().sum()\n",
    "print(f\"Number of Missing Values: {missing[missing > 0] if len(missing[missing > 0]) > 0 else \"No missing values\"}\")\n",
    "print(\"Missing Values\")\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe89e1-0ba7-41c2-b317-7bf142d4ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"## DUPLICATES\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb48e4b-8b15-4cdb-aa9a-b895dbb7b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_routes.duplicated(subset=['attributes.ROUTE_ID']).sum()\n",
    "print(f\"Duplicate route IDs: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328dc6f3-f99e-48a6-b648-b563cf551df3",
   "metadata": {},
   "source": [
    " # Cleaning: Bus Route Text Standardization & Type Conversions\n",
    "\n",
    " ## Data Wrangler Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f2f6a-690e-4cd7-83e8-fc9fe61db00e",
   "metadata": {},
   "source": [
    " ## Bus Route Cleaning Summary\n",
    " - inconsistent text formatting\n",
    " - missing colours\n",
    " - hex values missing #\n",
    " - standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56030647-7264-40f5-adb1-65ae19d1f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df_routes):\n",
    "    # Remove leading and trailing whitespace in columns: 'attributes.ROUTE_NAME', 'attributes.ROUTE_NUM' and 4 other columns\n",
    "    df_routes['attributes.ROUTE_NAME'] = df_routes['attributes.ROUTE_NAME'].str.strip()\n",
    "    df_routes['attributes.ROUTE_NUM'] = df_routes['attributes.ROUTE_NUM'].str.strip()\n",
    "    df_routes['attributes.ROUTE_ID'] = df_routes['attributes.ROUTE_ID'].str.strip()\n",
    "    df_routes['attributes.SHAPE_ID'] = df_routes['attributes.SHAPE_ID'].str.strip()\n",
    "    # add a hashtag in front of the route colour hex values \n",
    "    df_routes['attributes.ROUTE_COLOR'] = '#' + (df_routes['attributes.ROUTE_COLOR'].str.strip()).astype(str)\n",
    "    # Convert text to uppercase in column: 'attributes.ROUTE_NAME'\n",
    "    df_routes['attributes.ROUTE_NAME'] = df_routes['attributes.ROUTE_NAME'].str.upper()\n",
    "    # Replace missing values with \"FFFFFF\" in column: 'attributes.ROUTE_TEXT_COLOR'\n",
    "    df_routes = df_routes.fillna({'attributes.ROUTE_TEXT_COLOR': \"FFFFFF\"})\n",
    "    # add a hashtag in front of the route text colour hex values\n",
    "    df_routes['attributes.ROUTE_TEXT_COLOR'] = \"#\"+ (df_routes['attributes.ROUTE_TEXT_COLOR'].str.strip()).astype(str)\n",
    "     # Rename column 'attributes.SHAPE.LEN' to 'shape_length'\n",
    "    df_routes = df_routes.rename(columns={'attributes.SHAPE.LEN': 'shape_length'})\n",
    "    # Rename column 'attributes.ROUTE_NAME' to 'route_name'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_NAME': 'route_name'})\n",
    "    # Rename column 'attributes.ROUTE_NUM' to 'route_num'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_NUM': 'route_num'})\n",
    "    # Rename column 'attributes.ROUTE_ID' to 'route_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_ID': 'route_id'})\n",
    "    # Rename column 'attributes.SHAPE_ID' to 'shape_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.SHAPE_ID': 'shape_id'})\n",
    "    # Rename column 'attributes.ROUTE_COLOR' to 'route_color'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_COLOR': 'route_color'})\n",
    "    # Rename column 'attributes.ROUTE_TEXT_COLOR' to 'route_text_color'\n",
    "    df_routes = df_routes.rename(columns={'attributes.ROUTE_TEXT_COLOR': 'route_text_color'})\n",
    "    # Rename column 'geometry.paths' to 'geometry_paths'\n",
    "    df_routes = df_routes.rename(columns={'geometry.paths': 'geometry_paths'})\n",
    "    # Rename column 'attributes.OBJECTID' to 'object_id'\n",
    "    df_routes = df_routes.rename(columns={'attributes.OBJECTID': 'object_id'})\n",
    "    return df_routes\n",
    "\n",
    "clean_routes = clean_data(df_routes.copy())\n",
    "clean_routes.head()\n",
    "\n",
    "mr.Markdown(f\"**Cleaned routes:**{len(clean_routes)}\")\n",
    "clean_routes[['route_num', 'route_name', 'route_color']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375da1d-2e7a-48a8-bc26-4c9e2373c26e",
   "metadata": {},
   "source": [
    "  ## Transformation Step 1: Loading GTFS Schedule Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b5766-6965-44e4-a885-db677ed970e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**GTFS (General Transit Feed Specification)** provides detailed schedule information:\n",
    "\n",
    "```python\n",
    "# Load GTFS files\n",
    "stops_gtfs = pd.read_csv('raw_data/gtfs_data/stops.txt')\n",
    "routes_gtfs = pd.read_csv('raw_data/gtfs_data/routes.txt')\n",
    "trips_gtfs = pd.read_csv('raw_data/gtfs_data/trips.txt')\n",
    "times_gtfs = pd.read_csv('raw_data/gtfs_data/stop_times.txt')\n",
    "\n",
    "# Clean and standardize - sample\n",
    "stops_gtfs['stop_name'] = stops_gtfs['stop_name'].str.upper().str.strip()\n",
    "routes_gtfs['route_long_name'] = routes_gtfs['route_long_name'].str.upper().str.strip()\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96c420-51d3-46a3-a3b8-09870ad965a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GTFS data\n",
    "stops_gtfs = pd.read_csv('raw_data/gtfs_data/stops.txt')\n",
    "routes_gtfs = pd.read_csv('raw_data/gtfs_data/routes.txt')\n",
    "trips_gtfs = pd.read_csv('raw_data/gtfs_data/trips.txt')\n",
    "times_gtfs = pd.read_csv('raw_data/gtfs_data/stop_times.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47668728-28c0-49d5-ac1b-2f385acbc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(stops_gtfs):\n",
    "    # Convert text to uppercase in column: 'stop_name'\n",
    "    stops_gtfs['stop_name'] = stops_gtfs['stop_name'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'stop_name'\n",
    "    stops_gtfs['stop_name'] = stops_gtfs['stop_name'].str.strip()\n",
    "    return stops_gtfs\n",
    "\n",
    "stops_gtfs_clean = clean_data(stops_gtfs.copy())\n",
    "stops_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8131a-c3d7-4552-92b6-35560a5ae144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(routes_gtfs):\n",
    "    # Convert text to uppercase in column: 'route_long_name'\n",
    "    routes_gtfs['route_long_name'] = routes_gtfs['route_long_name'].str.upper()\n",
    "    # Remove leading and trailing whitespace in column: 'route_long_name'\n",
    "    routes_gtfs['route_long_name'] = routes_gtfs['route_long_name'].str.strip()\n",
    "    return routes_gtfs\n",
    "\n",
    "routes_gtfs_clean = clean_data(routes_gtfs.copy())\n",
    "routes_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f00663-37fc-4f97-96d1-ed2256c9f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(trips_gtfs):\n",
    "    # Remove leading and trailing whitespace in columns: 'route_id', 'service_id' and 2 other columns\n",
    "    trips_gtfs['route_id'] = trips_gtfs['route_id'].str.strip()\n",
    "    trips_gtfs['service_id'] = trips_gtfs['service_id'].str.strip()\n",
    "    trips_gtfs['trip_id'] = trips_gtfs['trip_id'].str.strip()\n",
    "    trips_gtfs['trip_headsign'] = trips_gtfs['trip_headsign'].str.strip()\n",
    "    # Convert text to uppercase in columns: 'service_id', 'trip_id', 'trip_headsign'\n",
    "    trips_gtfs['service_id'] = trips_gtfs['service_id'].str.upper()\n",
    "    trips_gtfs['trip_id'] = trips_gtfs['trip_id'].str.upper()\n",
    "    trips_gtfs['trip_headsign'] = trips_gtfs['trip_headsign'].str.upper()\n",
    "    return trips_gtfs\n",
    "\n",
    "trips_gtfs_clean = clean_data(trips_gtfs.copy())\n",
    "trips_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b83db0-8fab-487b-8e57-9fe724b96e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(times_gtfs):\n",
    "    # Convert text to uppercase in column: 'trip_id'\n",
    "    times_gtfs['trip_id'] = times_gtfs['trip_id'].str.upper()\n",
    "    # Replace '24' in hour position with '00'\n",
    "    times_gtfs['arrival_time'] = times_gtfs['arrival_time'].str.replace(r'^24', '00', regex=True)\n",
    "    times_gtfs['departure_time'] = times_gtfs['departure_time'].str.replace(r'^24', '00', regex=True)\n",
    "    # Remove leading and trailing whitespace in columns: 'trip_id', 'arrival_time', 'departure_time'\n",
    "    times_gtfs['trip_id'] = times_gtfs['trip_id'].str.strip()\n",
    "    times_gtfs['arrival_time'] = times_gtfs['arrival_time'].str.strip()\n",
    "    times_gtfs['departure_time'] = times_gtfs['departure_time'].str.strip()\n",
    "    return times_gtfs\n",
    "\n",
    "times_gtfs_clean = clean_data(times_gtfs.copy())\n",
    "times_gtfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8a492-7ae3-48a6-b67c-b3d785e1c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(f\"\"\"\n",
    "**GTFS Data Loaded:**\n",
    "- {len(stops_gtfs_clean)} stops\n",
    "- {len(routes_gtfs_clean)} routes\n",
    "- {len(trips_gtfs_clean)} trips\n",
    "- {len(times_gtfs_clean)} stop times\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bde2f5-8e48-48a4-a6fe-ba55543f0f31",
   "metadata": {},
   "source": [
    "  ## Transformation Step 2: Parsing Date/Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c07b8-40c1-4a25-856c-f42f5db2dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Problem:** Time data stored as strings (HH:MM:SS)\n",
    "\n",
    "**Solution:** Parse to datetime and derive time-based features\n",
    "\n",
    "```python\n",
    "# Parse time columns\n",
    "times_gtfs_clean['departure_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['departure_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "            \n",
    "times_gtfs_clean['departure_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['departure_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Derive hour and minute features\n",
    "times_gtfs_clean['arrival_hour'] = times_gtfs_clean['arrival_datetime'].dt.hour\n",
    "times_gtfs_clean['arrival_minute'] = times_gtfs_clean['arrival_datetime'].dt.minute\n",
    "times_gtfs_clean['departure_hour'] = times_gtfs_clean['departure_datetime'].dt.hour\n",
    "times_gtfs_clean['departure_minute'] = times_gtfs_clean['departure_datetime'].dt.minute\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670d3d4-f641-4b29-9ca6-c8acdba4463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse time columns\n",
    "times_gtfs_clean['arrival_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['arrival_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "times_gtfs_clean['departure_datetime'] = pd.to_datetime(\n",
    "    times_gtfs_clean['departure_time'], \n",
    "    format='%H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Derive hour of day\n",
    "times_gtfs_clean['arrival_hour'] = times_gtfs_clean['arrival_datetime'].dt.hour\n",
    "times_gtfs_clean['arrival_minute'] = times_gtfs_clean['arrival_datetime'].dt.minute\n",
    "times_gtfs_clean['departure_hour'] = times_gtfs_clean['departure_datetime'].dt.hour\n",
    "times_gtfs_clean['departure_minute'] = times_gtfs_clean['departure_datetime'].dt.minute\n",
    "\n",
    "mr.Markdown(\"**Parsed time data and derived hour/minute features**\")\n",
    "times_gtfs_clean[['arrival_time', 'arrival_datetime', 'arrival_hour', 'arrival_minute', 'departure_datetime', 'departure_time', 'departure_hour', 'departure_minute']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49c3b5-57e2-4085-9fe6-ce80d33c63ab",
   "metadata": {},
   "source": [
    "  ## Transformation Step 3: Merge/Join Operations\n",
    " - 1400 stops\n",
    " - Open Regina ASP.NET limit of 1000\n",
    " - GTFS data contains some fields\n",
    " - Imputation for the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b04ba7-3698-4fd6-98e9-200b271e4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_stops))\n",
    "print(len(stops_gtfs_clean))\n",
    "if len(stops_gtfs_clean) > len(clean_stops):\n",
    "    print(f\"There are {len(stops_gtfs_clean)-len(clean_stops)} missing stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d3d4b-3265-4fe7-a2b2-264e079a56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the join keys have the same data type\n",
    "clean_stops['stop_id'] = clean_stops['stop_id'].astype(str)\n",
    "stops_gtfs_clean['stop_id'] = stops_gtfs_clean['stop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4b5db-8036-4767-bd5e-c17f5752fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the missing stops\n",
    "missing_stops = stops_gtfs_clean[~stops_gtfs_clean['stop_id'].isin(clean_stops['stop_id'])]\n",
    "len(missing_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562ad59-b3fd-427d-8fd6-83d2846de3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "```python\n",
    "# Find stops in GTFS but not in geographic JSON data\n",
    "missing_stops = stops_gtfs_clean[\n",
    "    ~stops_gtfs_clean['stop_id'].isin(clean_stops['stop_id'])\n",
    "]\n",
    "\n",
    "# Merge datasets\n",
    "merged_stops = pd.concat([clean_stops, missing_stops], ignore_index=True, sort=False)\n",
    "\n",
    "# Impute missing street names from stop_name\n",
    "for index, stop in merged_stops.iterrows():\n",
    "    if pd.isna(stop['on_street']):\n",
    "        merged_stops.at[index, 'on_street'] = stop['stop_name'].split(' @')[0]\n",
    "    if pd.isna(stop['at_street']):\n",
    "        merged_stops.at[index, 'at_street'] = stop['stop_name'].split('@ ')[-1]\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf59682-4f93-4982-b900-faebeb4478ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing stops\n",
    "missing_stops = stops_gtfs_clean[~stops_gtfs_clean['stop_id'].isin(clean_stops['stop_id'])]\n",
    "\n",
    "mr.Markdown(f\"**Found stops in GTFS data:***{len(missing_stops)}\")\n",
    "\n",
    "# Merge\n",
    "merged_stops = pd.concat([clean_stops, missing_stops], ignore_index=True, sort=False)\n",
    "print(len(missing_stops), \"new stops added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b714f6b-ee7c-4e24-a866-706e570e3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(merged_stops):\n",
    "    for index, stop in merged_stops.iterrows():\n",
    "        if pd.isna(stop['on_street']):\n",
    "            merged_stops.at[index, 'on_street'] = (\n",
    "                str(merged_stops.at[index, 'stop_name']).split(' @')[0]\n",
    "            )\n",
    "        if pd.isna(stop['at_street']):\n",
    "            merged_stops.at[index, 'at_street'] = (\n",
    "                str(merged_stops.at[index, 'stop_name']).split('@ ')[-1]\n",
    "            )\n",
    "    return merged_stops\n",
    "\n",
    "merged_stops_clean = clean_data(merged_stops.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e75aa7-9f28-4ca9-b258-2cb20ed8efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(f\"**Total stops after merge:** {len(merged_stops_clean)} ({len(missing_stops)} added)\")\n",
    "merged_stops_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789615-1a97-4f83-8ebd-14f00b290806",
   "metadata": {},
   "source": [
    "  ## Transformation: Feature Derivation - Geographic Regions\n",
    "\n",
    " **Divide the city into four quadrants** based on approximate city centre coordinates (Albert St & Victoria Ave Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534a6ce-000b-4a46-aef4-a3c3ca1d6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "```python\n",
    "city_centre_lon = -104.618\n",
    "city_centre_lat = 50.447\n",
    "merged_stops_clean['region'] = ''\n",
    "\n",
    "# Assign quadrants (NE, NW, SE, SW)\n",
    "for stop in range(len(merged_stops_clean)):\n",
    "    if float(merged_stops_clean['lat'].iloc[stop]) > city_centre_lat:\n",
    "        if float(merged_stops_clean['lon'].iloc[stop]) > city_centre_lon:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"NE\"\n",
    "        else:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"NW\"\n",
    "    else:\n",
    "        if float(merged_stops_clean['lon'].iloc[stop]) > city_centre_lon:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"SE\"\n",
    "        else:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"SW\"\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6e0a1-710f-4cba-8de7-8886e970b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Bus Stops by Region\n",
    "city_centre_lon = -104.618\n",
    "city_centre_lat = 50.447\n",
    "merged_stops_clean['region'] = ''\n",
    "\n",
    "# Assign quadrants (NE, NW, SE, SW)\n",
    "for stop in range(len(merged_stops_clean)):\n",
    "    if float(merged_stops_clean['lat'].iloc[stop]) > city_centre_lat:\n",
    "        if float(merged_stops_clean['lon'].iloc[stop]) > city_centre_lon:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"NE\"\n",
    "        else:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"NW\"\n",
    "    else:\n",
    "        if float(merged_stops_clean['lon'].iloc[stop]) > city_centre_lon:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"SE\"\n",
    "        else:\n",
    "            merged_stops_clean.at[stop, 'region'] = \"SW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584ee74-bad5-4d21-ab50-0cc77e1103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"**Derived regional classifications for all stops**\")\n",
    "\n",
    "# Show distribution of stops per region\n",
    "region_counts = merged_stops_clean['region'].value_counts()\n",
    "mr.Markdown(f\"\"\"\n",
    "**Stop distribution by region:**\n",
    "- NW: {region_counts.get('NW', 0)} stops\n",
    "- NE: {region_counts.get('NE', 0)} stops\n",
    "- SW: {region_counts.get('SW', 0)} stops\n",
    "- SE: {region_counts.get('SE', 0)} stops\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394e6b7-f741-4561-8f1c-00fe7483ab75",
   "metadata": {},
   "source": [
    "  ## Transformation 5: Feature Derivation - Distance Calculations\n",
    "\n",
    "  - In ArcGIS/GIS systems, shape.len (shape length) represents the total length of the geometry in metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e490948-f1d4-41eb-90cc-f8faac6f133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "```python\n",
    "   # convert shape_length into km\n",
    "   clean_routes['route_distance_km'] = clean_routes['shape_length'] / 1000         \n",
    "```\n",
    "\"\"\")\n",
    "clean_routes['route_distance_km'] = clean_routes['shape_length'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820de52-85f1-419e-901f-fbb456f25acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_routes[['route_num', 'route_distance_km']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785ef78-4b1f-4096-a042-f29027ffd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Calculate distance from city centre** using coordinate geometry:\n",
    "\n",
    "```python\n",
    "# Approximate conversion: ~111 km per degree latitude, ~85 km per degree longitude at this latitude\n",
    "# Euclidean distance\n",
    "merged_stops_clean['distance_from_centre_km'] = np.sqrt(\n",
    "    ((merged_stops_clean['lat'].astype(float) - city_centre_lat) * 111)**2 + \n",
    "    ((merged_stops_clean['lon'].astype(float) - city_centre_lon) * 85)**2\n",
    ")\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36837c2-ffa3-4a94-b980-f2cd11e54b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate conversion: ~111 km per degree latitude, ~85 km per degree longitude at this latitude\n",
    "# Euclidean distance\n",
    "merged_stops_clean['distance_from_centre_km'] = np.sqrt(\n",
    "    ((merged_stops_clean['lat'].astype(float) - city_centre_lat) * 111)**2 + \n",
    "    ((merged_stops_clean['lon'].astype(float) - city_centre_lon) * 85)**2\n",
    ")\n",
    "\n",
    "mr.Markdown(\"**Calculated distance from city centre for all stops**\")\n",
    "print(\"\\nDistance statistics (km):\")\n",
    "print(merged_stops_clean['distance_from_centre_km'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209589f8-2ca0-43df-a009-a0c94ca490cd",
   "metadata": {},
   "source": [
    "  ## Transformation 6: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ad041-b698-497d-9dd3-29be55c4809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Aggregate stops by region** to understand service distribution:\n",
    "\n",
    "```python\n",
    "region_summary = clean_stops.groupby('region').agg({\n",
    "    'stop_id': 'count',\n",
    "    'distance_from_centre_km': ['mean', 'max']\n",
    "}).reset_index()\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4493a-f399-4f26-9180-620cc3c2b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_summary = merged_stops_clean.groupby('region').agg({\n",
    "    'stop_id': 'count',\n",
    "    'distance_from_centre_km': ['mean', 'max']\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61849a8f-11d0-4e05-92cb-2e3f14656ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_summary.columns = ['region', 'num_stops', 'avg_distance_km', 'max_distance_km']\n",
    "\n",
    "mr.Markdown(\"**Regional Summary Statistics:**\")\n",
    "region_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e753a3b-e8a1-4a20-b2ab-7496744a8223",
   "metadata": {},
   "source": [
    "  ## Transformation 7: Reshape (Pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bcaa2-b1d2-43ea-9960-27e2286f8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Pivot analysis:** How many stops does each route serve in each region?\n",
    "\n",
    "```python\n",
    "# Join stop times → trips → stops to get route-region relationships\n",
    "route_stops = (\n",
    "    times_gtfs_clean\n",
    "    .merge(trips_gtfs_clean[['trip_id', 'route_id']], on='trip_id')\n",
    "    .merge(merged_stops_clean[['stop_id', 'region']], left_on='stop_id', right_on='stop_id')\n",
    "    .groupby(['route_id', 'region'])\n",
    "    .size()\n",
    "    .reset_index(name='stop_count')\n",
    ")\n",
    "\n",
    "# Pivot to wide format\n",
    "route_region_pivot = route_stops.pivot(\n",
    "    index='route_id', \n",
    "    columns='region', \n",
    "    values='stop_count'\n",
    ").fillna(0)\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f131f65-0c5c-4483-96bb-04a1646c85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure join keys have the same dtype to avoid merge mismatches\n",
    "# Cast stop_id/trip_id/route_id to string on both sides of joins\n",
    "times_gtfs_clean['trip_id'] = times_gtfs_clean['trip_id'].astype(str)\n",
    "trips_gtfs_clean['trip_id'] = trips_gtfs_clean['trip_id'].astype(str)\n",
    "times_gtfs_clean['stop_id'] = times_gtfs_clean['stop_id'].astype(str)\n",
    "merged_stops_clean['stop_id'] = merged_stops_clean['stop_id'].astype(str)\n",
    "trips_gtfs_clean['route_id'] = trips_gtfs_clean['route_id'].astype(str)\n",
    "\n",
    "route_stops = (\n",
    "    times_gtfs_clean\n",
    "    .merge(trips_gtfs_clean[['trip_id', 'route_id']], on='trip_id')\n",
    "    .merge(merged_stops_clean[['stop_id', 'region']], left_on='stop_id', right_on='stop_id')\n",
    "    .groupby(['route_id', 'region'])\n",
    "    .size()\n",
    "    .reset_index(name='stop_count')\n",
    ")\n",
    "\n",
    "route_region_pivot = route_stops.pivot(\n",
    "    index='route_id', \n",
    "    columns='region', \n",
    "    values='stop_count'\n",
    ").fillna(0)\n",
    "\n",
    "mr.Markdown(\"**Routes by Region (Pivoted):**\")\n",
    "route_region_pivot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba8f48-c56c-4212-8f68-bb1e1fd41a70",
   "metadata": {},
   "source": [
    "  ## Profiling: Outliers & Cardinalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681a033-acb5-4d1c-9364-0181a0065081",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Outlier Detection:** Identify stops unusually far from city centre\n",
    "\n",
    "**Cardinality Analysis:** Count unique values in key dimensions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00982cdd-e9c2-4a17-b38c-cbba994aaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OUTLIERS ===\")\n",
    "print(\"Stops beyond 15km from city centre:\")\n",
    "outliers = merged_stops_clean[merged_stops_clean['distance_from_centre_km'] > 15]\n",
    "print(f\"Found {len(outliers)} outlier stops\")\n",
    "if len(outliers) > 0:\n",
    "    print(outliers[['stop_name', 'distance_from_centre_km']].head())\n",
    "\n",
    "print(\"\\n=== CARDINALITIES ===\")\n",
    "print(f\"Unique stops: {merged_stops_clean['stop_id'].nunique()}\")\n",
    "print(f\"Unique routes: {routes_gtfs_clean['route_id'].nunique()}\")\n",
    "print(f\"Unique regions: {merged_stops_clean['region'].nunique()}\")\n",
    "print(f\"Unique trips: {trips_gtfs_clean['trip_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b5cd4-d989-49f2-ab01-c6ec44c6f919",
   "metadata": {},
   "source": [
    "  ## Before/After Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61674f-f6a4-47d2-895e-472faa73b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "### Quantitative comparison of data quality improvements:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba4071-7056-4b60-8cdb-71da33981b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BEFORE (Raw Data)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Stop rows: {len(df_stops)}\")\n",
    "print(f\"Route rows: {len(df_routes)}\")\n",
    "print(f\"Missing ATSTREET: {df_stops['attributes.ATSTREET'].isnull().sum()}\")\n",
    "print(f\"Coordinate type: {df_stops['attributes.LAT'].dtype} (string)\")\n",
    "print(f\"Stop ID type: {df_stops['attributes.STOP_ID'].dtype} (string)\")\n",
    "print(f\"Features: 8 columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AFTER (Cleaned & Transformed)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Stop rows: {len(merged_stops_clean)} (+{len(missing_stops)} from GTFS)\")\n",
    "print(f\"Route rows: {len(clean_routes)}\")\n",
    "print(f\"Missing ATSTREET: {merged_stops_clean['at_street'].isnull().sum()}\")\n",
    "print(f\"Coordinate type: {merged_stops_clean['lat'].dtype} (numeric-ready)\")\n",
    "print(f\"Stop ID type: {merged_stops_clean['stop_id'].dtype} (int32)\")\n",
    "print(f\"Features: {len(merged_stops_clean.columns)} columns\")\n",
    "print(f\"New derived features: region, route_distance_km, distance_from_centre_km\")\n",
    "print(f\"Parsed time features: arrival_hour, arrival_minute, departure_hour, departure_minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410b6cc-5640-436a-a0cd-3aaffbd13045",
   "metadata": {},
   "source": [
    "  ## Visualization: Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20881152-dd0d-4362-ad21-c463011a3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Interactive map** showing all bus stops with hover information:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc0b32-17d0-415e-9dc5-0e9df0152d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from the Plotly documentation https://plotly.com/python/tile-scatter-maps/#multiple-markers\n",
    "stop_fig = go.Figure(go.Scattermap(\n",
    "    lat=clean_stops['lat'],\n",
    "    lon=clean_stops['lon'],\n",
    "    mode='markers',\n",
    "    marker=go.scattermap.Marker(size=9, color='blue'),\n",
    "    text=clean_stops['stop_name'],\n",
    "    hovertemplate='<b>%{text}</b><extra></extra>'\n",
    "))\n",
    "\n",
    "stop_fig.update_layout(\n",
    "    title=\"Regina Transit Stops\",\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    map=dict(\n",
    "        bearing=0,\n",
    "        centre=dict(lat=50.447992743219615, lon=-104.61228441057489),\n",
    "        pitch=0,\n",
    "        zoom=11\n",
    "    ),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "stop_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c11447-2fee-427d-b52b-e4b1e30fdb24",
   "metadata": {},
   "source": [
    "  ## Visualization: Route Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53844b-d0da-401d-af8e-e79b582c3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "**Adding route geometry** with coordinate transformation from UTM to lat/lon:\n",
    "\n",
    "```python\n",
    "# Transform UTM coordinates to lat/lon\n",
    "transformer = Transformer.from_crs(\"EPSG:26913\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "for coordinate in path:\n",
    "    lon, lat = transformer.transform(coordinate[0], coordinate[1])\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb43bd7-0fe2-43a4-b745-eb79a291dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs(\"EPSG:26913\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "route_idx = 0\n",
    "route_name = clean_routes['route_name'].iloc[route_idx]\n",
    "route_geometry = clean_routes['geometry_paths'].iloc[route_idx]\n",
    "route_colour = clean_routes['route_color'].iloc[route_idx]\n",
    "\n",
    "# Transform coordinates\n",
    "all_lons = []\n",
    "all_lats = []\n",
    "\n",
    "for path in route_geometry:\n",
    "    for coordinate in path:\n",
    "        lon, lat = transformer.transform(coordinate[0], coordinate[1])\n",
    "        all_lons.append(lon)\n",
    "        all_lats.append(lat)\n",
    "    all_lons.append(None)\n",
    "    all_lats.append(None)\n",
    "\n",
    "# Add route to map\n",
    "stop_fig.add_trace(go.Scattermap(\n",
    "    lon=all_lons,\n",
    "    lat=all_lats,\n",
    "    mode='lines',\n",
    "    line=dict(width=3, color=route_colour),\n",
    "    name=route_name,\n",
    "    hovertemplate=f'<b>{route_name}</b><extra></extra>'\n",
    "))\n",
    "\n",
    "stop_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953f2bd-d85c-49c4-8cd7-323736665afc",
   "metadata": {},
   "source": [
    "  ## Summary of Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ac527-4917-434d-8b3b-34789c6b974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "### Completed Transformations (8 operations across 5 categories):\n",
    "\n",
    "**1. Type Fixes & Parsing**\n",
    "- Converted stop_id from string to int32\n",
    "- Parsed arrival/departure times to datetime\n",
    "- Derived hour and minute features\n",
    "\n",
    "**2. Text Cleanup**\n",
    "- Stripped whitespace from all text columns\n",
    "- Converted to uppercase for consistency\n",
    "- Fixed malformed addresses\n",
    "\n",
    "**3. Missing Data Handling**\n",
    "- Imputed missing ATSTREET values\n",
    "- Generated street names from stop names for GTFS stops\n",
    "\n",
    "**4. Join/Merge**\n",
    "- Merged geographic stops with GTFS schedule data\n",
    "- Joined stop times → trips → routes → stops\n",
    "\n",
    "**5. Feature Derivation**\n",
    "- Created regional classifications (NE, NW, SE, SW)\n",
    "- Calculated distance from city centre\n",
    "- Calculate total distance of each route\n",
    "\n",
    "**6. Aggregation**\n",
    "- Summarized stops by region with statistics\n",
    "\n",
    "**7. Reshape**\n",
    "- Pivoted route-region stop counts to wide format\n",
    "\n",
    "**8. Coordinate Transformation**\n",
    "- Converted UTM to lat/lon for visualization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80895e0-0f55-4735-8ac4-03591744b4f4",
   "metadata": {},
   "source": [
    "  ## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7ae89-649a-40ad-ac27-3aaa0add0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.Markdown(\"\"\"\n",
    "### How to reproduce this analysis:\n",
    "\n",
    "**1. Install dependencies:**\n",
    "```bash\n",
    "pip install pandas plotly pyproj numpy jupyter\n",
    "```\n",
    "\n",
    "**2. Directory structure:**\n",
    "```\n",
    "project/\n",
    "├── presentation.ipynb\n",
    "├── transit_data.ipynb\n",
    "├── raw_data/\n",
    "│   ├── yqrStops.json\n",
    "│   ├── yqrRoutes.json\n",
    "│   └── gtfs_data/\n",
    "│       ├── stops.txt\n",
    "│       ├── routes.txt\n",
    "│       ├── trips.txt\n",
    "│       └── stop_times.txt\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "**3. Run notebook:**\n",
    "- Command line: jupyter notebook transit_data.ipynb\n",
    "- In VS Code: Execute all cells sequentially\n",
    "- Or run with Mercury: `mercury run presentation.ipynb`\n",
    "\n",
    "## Tool Versions\n",
    "\n",
    "- Python: 3.14.0\n",
    "- pandas: 2.3.3\n",
    "- numpy: 2.3.5\n",
    "- plotly: 6.5.0\n",
    "- pyproj: 3.7.2\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
